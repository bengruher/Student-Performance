{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e8131a2",
   "metadata": {},
   "source": [
    "# Modeling on Amazon Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39f7068",
   "metadata": {},
   "source": [
    "This notebook demonstrates the process of training and deploying our best machine learning model (discovered locally) in the cloud using Amazon Sagemaker. We will be using Sagemaker's \"script mode\" and using Sagemaker's prebuilt SKLearn container for both our data preprocessing steps and our modeling. We will be creating an inference pipeline that includes the data transformation operations as well as the training/inference of the machine learning model. This inference pipeline will contain two Sagemaker \"models\". The first model is responsible for the data preprocessing. In Sagemaker, we deploy this much the same way as an estimator. However, we will override some of the Sagemaker functions such as ```input_fn```, ```output_fn```, and ```predict_fn``` to make sure that we are using the transform operation as opposed to predicting a value. We will fit the data preprocessor to the training data and persist the scikit-learn object(s) to S3. When new data is sent to the pipeline for inference, we load those pre-fit objects and use them to transform the data before it is sent to our estimator.\n",
    "\n",
    "Note: In order to run this example, you will need to be using an Amazon Sagemaker notebook and your notebook will need to have proper IAM permissions to your AWS environment. This includes write access to Amazon Simple Storage Service (S3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452b0046",
   "metadata": {},
   "source": [
    "### Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f67e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "import pandas as pd\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "from sagemaker.predictor import csv_serializer, RealTimePredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba9f69",
   "metadata": {},
   "source": [
    "### Configure Sagemaker environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d4c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe4c740",
   "metadata": {},
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dc9b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_HYPERPARAMS = {'max_depth': 2, 'min_samples_leaf': 7, 'min_samples_split': 2, 'n_estimators': 50}\n",
    "LOCAL_MODE = True\n",
    "TRAINING_INSTANCE_TYPE = 'local' if LOCAL_MODE == True else 'ml.c5.xlarge'\n",
    "INFERENCE_INSTANCE_TYPE = 'ml.c5.xlarge'\n",
    "sagemaker_session = sagemaker.LocalSession() if LOCAL_MODE == True else sagemaker.Session()\n",
    "DATA_DIRECTORY = 'data/'\n",
    "S3_BUCKET = sagemaker_session.default_bucket()\n",
    "S3_PREFIX = 'student-performance-sagemaker'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd9c90b",
   "metadata": {},
   "source": [
    "### Write preprocessing and training scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10afca93",
   "metadata": {},
   "source": [
    "The steps defined in the below scripts were developed during our initial exploration. See the root directory for more detail on how we developed these steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sagemaker_preprocessing.py\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sagemaker_containers import _encoders as encoders\n",
    "from sagemaker_containers import _worker as worker\n",
    "\n",
    "# the following may be required in order to get the SKLearn container to deploy properly\n",
    "# see the following issue: https://github.com/aws/sagemaker-python-sdk/issues/648\n",
    "# module_path = os.path.abspath('/opt/ml/code')\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "\n",
    "feature_columns_names = ['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu',\n",
    "       'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime',\n",
    "       'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery',\n",
    "       'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc',\n",
    "       'Walc', 'health', 'absences', 'G1', 'G2']\n",
    "\n",
    "label_column = 'G3'\n",
    "\n",
    "numeric_cols_to_keep = ['age', 'Medu', 'traveltime', 'studytime', 'failures', 'goout', 'Dalc', 'absences']\n",
    "nominal_cols_to_keep = ['address', 'Fjob', 'guardian', 'higher', 'internet', 'romantic']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Take the set of files and read them all into a single pandas dataframe\n",
    "    input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(args.train, \"train\"))\n",
    "    raw_data = [ pd.read_csv(\n",
    "        file, \n",
    "        header=0, \n",
    "        index_col=0) for file in input_files ]\n",
    "    concat_data = pd.concat(raw_data)\n",
    "        \n",
    "    preprocessor = ColumnTransformer(\n",
    "         transformers = [(\"numeric\", MinMaxScaler(), numeric_cols_to_keep),\n",
    "                         (\"nominal\", OneHotEncoder(drop='if_binary', handle_unknown='error'), nominal_cols_to_keep)],\n",
    "         remainder = 'drop',\n",
    "         n_jobs = -1\n",
    "    )\n",
    "    \n",
    "    preprocessor.fit(concat_data)\n",
    "    \n",
    "    joblib.dump(preprocessor, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "    \n",
    "    \n",
    "def input_fn(input_data, content_type):\n",
    "    \"\"\"Parse input data payload\n",
    "\n",
    "    This function is used by Amazon Sagemaker only during inference.\n",
    "    We will only allow text/csv format. Since we need to process both labelled\n",
    "    and unlabelled data we first determine whether the label column is present\n",
    "    by looking at how many columns were provided.\n",
    "    \"\"\"\n",
    "    if content_type == 'text/csv':\n",
    "        # Read the raw input data as CSV\n",
    "        # We need to use StringIO because the input_data will be the actual csv data, not the filename\n",
    "        df = pd.read_csv(StringIO(input_data), index_col=0)\n",
    "        \n",
    "        if len(df.columns) == len(feature_columns_names) + 1:\n",
    "            # This is a labelled example, includes the G3 label\n",
    "            df.columns = feature_columns_names + [label_column]\n",
    "        elif len(df.columns) == len(feature_columns_names):\n",
    "            # This is an unlabelled example.\n",
    "            df.columns = feature_columns_names\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(\"{} not supported by script!\".format(content_type))\n",
    "    \n",
    "    \n",
    "def output_fn(prediction, accept):\n",
    "    \"\"\"Format prediction output\n",
    "\n",
    "    This function is used by Amazon Sagemaker only during inference.\n",
    "    The default accept/content-type between containers for serial inference is JSON.\n",
    "    We also want to set the ContentType or mimetype as the same value as accept so the next\n",
    "    container can read the response payload correctly.\n",
    "    \"\"\"\n",
    "    if accept == \"application/json\":\n",
    "        instances = []\n",
    "        for row in prediction.tolist():\n",
    "            instances.append({\"features\": row})\n",
    "\n",
    "        json_output = {\"instances\": instances}\n",
    "\n",
    "        return worker.Response(json.dumps(json_output), accept, mimetype=accept) # we use the Sagemaker container helper classes to return the proper types\n",
    "#         return json.dumps(json_output)\n",
    "    elif accept == 'text/csv':\n",
    "        return worker.Response(encoders.encode(prediction, accept), accept, mimetype=accept) # we use the Sagemaker container helper classes to return the proper types\n",
    "    else:\n",
    "        raise RuntimeError(\"{} accept type is not supported by this script.\".format(accept))\n",
    "        \n",
    "        \n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"Preprocess input data\n",
    "\n",
    "    We implement this because the default predict_fn uses .predict(), but our model is a preprocessor\n",
    "    so we want to use .transform().\n",
    "    \"\"\"\n",
    "    features = model.transform(input_data)\n",
    "    \n",
    "    # if labels were passed in, we need to add them back to the dataset because ColumnTransformer will remove them\n",
    "    if label_column in input_data:\n",
    "        # Return the label (as the first column) and the set of features.\n",
    "        return np.insert(features, 0, input_data[label_column], axis=1)\n",
    "    else:\n",
    "        # Return only the set of features\n",
    "        return features\n",
    "    \n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialize fitted model\n",
    "    \"\"\"\n",
    "    preprocessor = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14119179",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sagemaker_modeling.py\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    \n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script\n",
    "    parser.add_argument(\"--max-depth\", type=int, default=2)\n",
    "    parser.add_argument(\"--min-samples-leaf\", type=int, default=7)\n",
    "    parser.add_argument(\"--min-samples-split\", type=int, default=2)\n",
    "    parser.add_argument(\"--n-estimators\", type=int, default=50)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Take the set of files and read them all into a single pandas dataframe\n",
    "    train_dir = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]\n",
    "    if len(train_dir) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(args.train, \"train\"))\n",
    "        \n",
    "    input_files = list()\n",
    "    for item in train_dir:\n",
    "        if os.path.isdir(item): # item in train folder may be a directory with files (directory can't be read by Pandas)\n",
    "            subitems = os.listdir(item)\n",
    "            for file in subitems:\n",
    "                input_files.append(item + '/' + file) # add full path to filename\n",
    "        else:\n",
    "            input_files.append(item)\n",
    "    print('Input files: ', input_files)\n",
    "    raw_data = [ pd.read_csv(file) for file in input_files ]\n",
    "    concat_data = pd.concat(raw_data)\n",
    "        \n",
    "    # separate features and target variable\n",
    "    X = concat_data[concat_data.columns[1:]]\n",
    "    y = concat_data[concat_data.columns[0]]\n",
    "    \n",
    "    hyperparameters = {\n",
    "        \"max_depth\": args.max_depth,\n",
    "        \"verbose\": 1,  # show all logs\n",
    "        \"min_samples_leaf\": args.min_samples_leaf,\n",
    "        \"min_samples_split\": args.min_samples_split,\n",
    "        \"n_estimators\": args.n_estimators,\n",
    "    }\n",
    "    \n",
    "    print(\"Training the regressor\")\n",
    "    model = RandomForestRegressor()\n",
    "    model.set_params(**hyperparameters)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    joblib.dump(model, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Deserialized and return fitted model\n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    model = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6a7de0",
   "metadata": {},
   "source": [
    "### Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a247345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will read in the two data files and concatenate them into a single file to simplify the upload process to S3\n",
    "math_data = pd.read_csv(filepath_or_buffer = '../data/student-mat.csv', sep=';', header=0)\n",
    "port_data = pd.read_csv(filepath_or_buffer = '../data/student-por.csv', sep=';', header=0)\n",
    "df = pd.concat([math_data, port_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef76391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we make a data directory in our AWS folder containing our data destined for Sagemaker\n",
    "!mkdir -p {DATA_DIRECTORY}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d13ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path_or_buf = DATA_DIRECTORY + 'train.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc9233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = sagemaker_session.upload_data(\n",
    "    path='{}/{}'.format(DATA_DIRECTORY, 'train.csv'), \n",
    "    bucket=S3_BUCKET,\n",
    "    key_prefix='{}/{}'.format(S3_PREFIX, 'train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54329f65",
   "metadata": {},
   "source": [
    "### Fit data preprocesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d196e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessing_script_path = 'sagemaker_preprocessing.py'\n",
    "\n",
    "sklearn_preprocessor = SKLearn(\n",
    "    entry_point=preprocessing_script_path,\n",
    "    role=role,\n",
    "    instance_type=TRAINING_INSTANCE_TYPE,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    py_version=\"py3\",\n",
    "    framework_version=\"0.23-1\")\n",
    "\n",
    "sklearn_preprocessor.fit({'train': train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95793838",
   "metadata": {},
   "source": [
    "### Transform train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bac4b87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a SKLearn Transformer from the trained SKLearn Estimator\n",
    "transformer = sklearn_preprocessor.transformer(\n",
    "    instance_count=1, \n",
    "    instance_type=TRAINING_INSTANCE_TYPE,\n",
    "    accept = 'text/csv')\n",
    "#     accept = 'application/json') # since we support both csv and JSON, we can choose to accept either one\n",
    "\n",
    "# Preprocess training input\n",
    "transformer.transform(train_input, content_type='text/csv')\n",
    "print('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\n",
    "transformer.wait()\n",
    "preprocessed_train = transformer.output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7618ff60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Processed data written to: ', preprocessed_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6599bdc",
   "metadata": {},
   "source": [
    "### Creating estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a091371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_script_path = 'sagemaker_modeling.py'\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=modeling_script_path,\n",
    "    role=role,\n",
    "    instance_type=TRAINING_INSTANCE_TYPE,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    py_version=\"py3\",\n",
    "    framework_version=\"0.23-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ae88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_data = sagemaker.inputs.TrainingInput(\n",
    "    preprocessed_train, # location of preprocessed data in S3\n",
    "    distribution='FullyReplicated',\n",
    "    content_type='text/csv', \n",
    "    s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': preprocessed_train_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a66387",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sklearn_estimator.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f079f9",
   "metadata": {},
   "source": [
    "### Create Sagemaker inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9350c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_step = sklearn_preprocessor.create_model()\n",
    "estimator_step = sklearn_estimator.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a484f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Student-Performance-Pipeline-Model'\n",
    "endpoint_name = 'Student-Performance-Pipeline-Endpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30662224",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model = PipelineModel(\n",
    "    name=model_name, \n",
    "    role=role, \n",
    "    models=[\n",
    "        preprocessor_step, \n",
    "        estimator_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469bc8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model.deploy(initial_instance_count=1, instance_type=INFERENCE_INSTANCE_TYPE, endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8734628a",
   "metadata": {},
   "source": [
    "### Test inference for the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eb3b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = 'GP, M, 17, R, GT3, T, 3, 4, services, health, \"home\", \"mother\", 2, 2, 1, no, yes, no, yes, no, yes, yes, no, 5, 2, 2, 1, 1, 5, 5, 17, 18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e69bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = RealTimePredictor(\n",
    "    endpoint=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=csv_serializer,\n",
    "    content_type='text/csv',\n",
    "    accept='application/json')\n",
    "\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3191c2b4",
   "metadata": {},
   "source": [
    "### Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c43b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = sagemaker_session.boto_session.client('sagemaker')\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602a03b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
